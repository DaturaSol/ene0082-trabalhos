{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db3f31b",
   "metadata": {},
   "source": [
    "#   Regressão Linear na qualidade de vinho.  \n",
    "\n",
    "> **Autor**: *Gabriel M. S. O.*  \n",
    "> **Matrícula**: 190042656"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c61fa5",
   "metadata": {},
   "source": [
    "\n",
    "##  Intruções para rodar o codigo.\n",
    "\n",
    "Estarei usando o package manager `Poetry`,\n",
    "uma vez que deixa minha instancia python\n",
    "padrão.  \n",
    "É a forma mais proxima de um docker container\n",
    "sem a necessidade de usar docker...  \n",
    "\n",
    "[Documentação](https://python-poetry.org/)  \n",
    "\n",
    "### Instalar poetry.\n",
    "\n",
    "\n",
    "Você pode instalar o poetry de duas maneiras.  \n",
    "\n",
    "#### PIP\n",
    "Mais simples seria usando pip,\n",
    "contudo a documentação do poetry não\n",
    "recomenda a instalação desta maneira.\n",
    "  \n",
    "```bash\n",
    "pip install poetry\n",
    "```\n",
    "\n",
    "#### Seguindo a [documentação](https://python-poetry.org/docs/#installation).\n",
    "\n",
    "```bash\n",
    "curl -sSL https://install.python-poetry.org | python3 - --version 2.1.1\n",
    "```\n",
    "\n",
    "### Instalar dependencias.\n",
    "\n",
    "Apenas rode.\n",
    "\n",
    "```bash\n",
    "poetry env activate && \\\n",
    "poetry install --no-root\n",
    "```\n",
    "\n",
    "### Rodar o codigo.\n",
    "\n",
    "Por fim use o virtual enviroment criado,\n",
    "poetry cuidara das dependencias.  \n",
    "\n",
    "Olhe em: [PyProject](pyproject.toml)\n",
    "\n",
    "##  Não desejo rodar o codigo.\n",
    "\n",
    "Tudo bem, PDF do projeto esta na pasta [out](/out/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfd7ee5",
   "metadata": {},
   "source": [
    "## Escopo do projeto\n",
    "\n",
    "Meu objetivo com esse projeto sera aprender\n",
    "a utilizar a biblioteca `PyTorch` com mais\n",
    "familiaridade, também implementar os conceitos\n",
    "de regressão linear na mão, logo não desejo \n",
    "depender do modulo `d2l`. Uma vez que vejo\n",
    "como uma muleta no aprendizado.\n",
    "\n",
    "### Requisitos do projeto\n",
    "\n",
    "*   Qual o erro absoluto medio do meu preditor ?\n",
    "    *   Este error é melhor que o preditor trivial ?\n",
    "    *   Quão melhor que o preditor trivial ?\n",
    "    *   Compare a acuracia contra o preditor trivial.\n",
    "*   Qual a correlação entre a saida desejada e obtida ?\n",
    "*   Variar número de epocas, minibatch, taxa de aprendizado\n",
    "*   Normalização das entradas\n",
    "*   Tentar eliminar variaveis pouco significantes (optimizar o modelo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3e5e62",
   "metadata": {},
   "source": [
    "##  Entendendo o conjunto de dados.\n",
    "\n",
    "Para montar nosso modelo primeiro precisamos\n",
    "entender com que tipo de informações estamos trabalhando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0f29702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "fixed acidity\n",
      "volatile acidity\n",
      "citric acid\n",
      "residual sugar\n",
      "chlorides\n",
      "free sulfur dioxide\n",
      "total sulfur dioxide\n",
      "density\n",
      "pH\n",
      "sulphates\n",
      "alcohol\n",
      "quality\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"./data\")\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FILE_NAME = \"winequalityN.csv\"\n",
    "FILE_PATH = DATA_PATH / FILE_NAME\n",
    "\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "print(*df.columns, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31912f5",
   "metadata": {},
   "source": [
    "Podemos observar diferentes variaveis, \n",
    "vamos entender cada uma melhor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "282c757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas.api.types as pdtypes\n",
    "\n",
    "\n",
    "def get_column_summary(series: pd.Series) -> dict:\n",
    "    dtype = series.dtype\n",
    "    column_info = {\"dtype\": str(dtype)}\n",
    "\n",
    "    if series.empty:\n",
    "        return column_info\n",
    "\n",
    "    if pdtypes.is_numeric_dtype(dtype):\n",
    "        max_val = series.max()\n",
    "        min_val = series.min()\n",
    "        avr = series.sum() / series.__len__()\n",
    "\n",
    "        if pdtypes.is_integer_dtype(dtype):\n",
    "            column_info[\"avr\"] = round(avr)\n",
    "        else:\n",
    "            column_info[\"avr\"] = round(avr, ndigits=5)\n",
    "\n",
    "        if hasattr(max_val, \"item\"):\n",
    "            column_info[\"max\"] = max_val.item()\n",
    "        else:\n",
    "            column_info[\"max\"] = max_val\n",
    "\n",
    "        if hasattr(min_val, \"item\"):\n",
    "            column_info[\"min\"] = min_val.item()\n",
    "        else:\n",
    "            column_info[\"min\"] = min_val\n",
    "\n",
    "    elif (\n",
    "        pdtypes.is_object_dtype(dtype)\n",
    "        or pdtypes.is_string_dtype(dtype)\n",
    "        or pdtypes.is_bool_dtype(dtype)\n",
    "    ):\n",
    "        unique_values = series.unique()\n",
    "        column_info[\"unique\"] = unique_values.tolist()\n",
    "\n",
    "    return column_info\n",
    "\n",
    "\n",
    "column_dict = {col: get_column_summary(df[col]) for col in df.columns}\n",
    "\n",
    "data_dict = {\"size\": df.__len__(), \"columns\": column_dict}\n",
    "\n",
    "data_dict_dumped = json.dumps(data_dict, indent=2)\n",
    "\n",
    "output_dir_path = Path(\"./out\")\n",
    "output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_file_path = output_dir_path / \"data_info.json\"\n",
    "\n",
    "with output_file_path.open(\"w\") as file:\n",
    "    file.write(data_dict_dumped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a435b6c",
   "metadata": {},
   "source": [
    "Escrevi as informações em [`./out/data_info.json`](./out/data_info.json).  \n",
    "\n",
    "Com isto podemos ver cada variavel, o score final *quality*, da se a entender\n",
    "que é uma nota que vai de 0 a 10, mas não sei se seria apropriado\n",
    "fazer essa suposição, uma vez que não tenho certeza, logo usarei os\n",
    "valores maxímos e minimos do nosso **data set** para serem \n",
    "o chão e o teto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a7919d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"dtype\": \"int64\",\n",
      "  \"avr\": 6,\n",
      "  \"max\": 9,\n",
      "  \"min\": 3\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(data_dict[\"columns\"][\"quality\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1348d3",
   "metadata": {},
   "source": [
    "## Plano de ação\n",
    "\n",
    "Uma vez que possuimos diferentes tipos de vinho, minha ideia\n",
    "seria criar um set de pesos para cada tipo. Também\n",
    "vamos concatenar o *bias* no data set. Porque eu quero...\n",
    "\n",
    "Logo nossa equação seria algo do genero.\n",
    "\n",
    "$$ \n",
    "y^{i} = X_{j\\ k}^{i}\\ w^{j\\ k} \n",
    "$$\n",
    "\n",
    "*   $N$ Tamanho do Data Set; \n",
    "*   $i$ Indice de onde estamos no nosso batch;\n",
    "*   $j$ Indice do tamanho do vetor de peso $\\vec{w}$;\n",
    "*   $k$ Indice do tipo do vinho.\n",
    "\n",
    "**Aqui começa minha solução**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f458f19",
   "metadata": {},
   "source": [
    "## Solução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29997a7d",
   "metadata": {},
   "source": [
    "#### Variaveis globais\n",
    "\n",
    "Não gosto de variaveis globais, mas como estamos usando\n",
    "o jupyter ate que desce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d11302",
   "metadata": {},
   "source": [
    "### Bibliotecas a serem usadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e086565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas #  para lidar com o Data Set\n",
    "import torch #  Erm what the sigma\n",
    "import numpy # erm What the sigmar\n",
    "from pathlib import Path #  Gosto dessa blibioteca...\n",
    "from torch.utils.data import TensorDataset, DataLoader #  Realização de batches \n",
    "from sklearn.preprocessing import StandardScaler #  Para normalizar nossos dados.\n",
    "from sklearn.model_selection import train_test_split #  Dividir nossos dados em teste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5362cb2",
   "metadata": {},
   "source": [
    "### Carregar Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c613468",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"./data\")\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_FILE_NAME = \"winequalityN.csv\"\n",
    "DATA_FILE_PATH = DATA_PATH / DATA_FILE_NAME\n",
    "\n",
    "df = pandas.read_csv(DATA_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ecb79",
   "metadata": {},
   "source": [
    "#### Tratamento dos dados\n",
    "\n",
    "Usar a função `get_dummies` para\n",
    "gerar uma nova tabela, contendo\n",
    "informações boleanas, se o conteudo\n",
    "se encaixa em um determinado tipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c403f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.get_dummies(df, columns=[\"type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6f4e56",
   "metadata": {},
   "source": [
    "Vamos avalair quantos porcento dos dados estão faltando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cdebc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m missing_pct = \u001b[43mdf\u001b[49m.isnull().sum()\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(missing_pct.sort_values(ascending=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "missing_pct = df.isna().mean() * 100\n",
    "print(missing_pct.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5a17a",
   "metadata": {},
   "source": [
    "Dado que faltam muitos poucos dados, vou substituir estes\n",
    "pela media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a23bc11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5db14cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7954440712928772\n",
      "0.4365091621875763\n"
     ]
    }
   ],
   "source": [
    "quality = df['quality']\n",
    "quality = quality.to_numpy()\n",
    "\n",
    "quality_mean = quality.mean().__round__()\n",
    "quality_mean = numpy.full_like(quality, fill_value=quality_mean)\n",
    "\n",
    "quality = torch.tensor(quality, dtype=torch.float32)\n",
    "quality_mean = torch.tensor(quality_mean, dtype=torch.float32)\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "loss = mse_loss(quality_mean, quality)\n",
    "accu = (quality_mean == quality).float().mean().item()\n",
    "print(loss.item())\n",
    "print(accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5e96c0",
   "metadata": {},
   "source": [
    "Separar as colunas de interesse\n",
    "\n",
    "*   Colunas númericas:  \n",
    "    Contem as informações\n",
    "    que usaremos nos pesos.\n",
    "*   Colunas de tipo:  \n",
    "    Representam efetivamente\n",
    "    se aquela linha de informações\n",
    "    bate com o tipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86f9afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_columns(\n",
    "    df: pandas.DataFrame,\n",
    ") -> tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]:\n",
    "    \"\"\"\"\"\"\n",
    "    cols_names = df.columns\n",
    "    type_mask = cols_names.str.startswith(\"type_\")\n",
    "\n",
    "    type_cols_names = cols_names[type_mask]\n",
    "    numeric_cols_names = cols_names[~type_mask & (cols_names != \"quality\")]\n",
    "\n",
    "    X_type_values = df[type_cols_names].values.astype(\n",
    "        \"float32\"\n",
    "    )  # Tensor (N, type_cols_size), type_cols_size = 2\n",
    "    # print(X_type_values.shape)\n",
    "    # Representam efetivamente se aquela linha de informações bate com o tipo.\n",
    "\n",
    "    X_numeric_values = df[numeric_cols_names].values.astype(\n",
    "        \"float32\"\n",
    "    )  # Tensor (N, numeric_cols_size), numeric_cols_size = 12, inclui os bias\n",
    "    # Contem as informações que usaremos nos pesos.\n",
    "    # print(X_numeric_values.shape)\n",
    "\n",
    "    y = (\n",
    "        df[\"quality\"].values.astype(\"float32\").reshape(-1, 1)\n",
    "    )  # Tensor (N,).reshape(-1, 1) -> Tensor (N, 1)\n",
    "    # print(y.shape)\n",
    "\n",
    "    return X_numeric_values, X_type_values, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c2a97",
   "metadata": {},
   "source": [
    "#### Normalização das Entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "470dd312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_values(X_numeric_values: numpy.ndarray) -> numpy.ndarray:\n",
    "    \"\"\"\"\"\"\n",
    "    X_num_val_norm = StandardScaler().fit_transform(X_numeric_values)\n",
    "    X_num_val_norm_with_bias = numpy.concatenate(\n",
    "        [X_num_val_norm, numpy.ones((X_num_val_norm.shape[0], 1), dtype=\"float32\")],\n",
    "        axis=1,\n",
    "    )\n",
    "    return X_num_val_norm_with_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c9555",
   "metadata": {},
   "source": [
    "#### Divisão em Sets de Treinamento e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbf909ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_sets(\n",
    "    X_numeric_values: numpy.ndarray,\n",
    "    X_type_values: numpy.ndarray,\n",
    "    y: numpy.ndarray,\n",
    "    test_data_size: float,\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\"\"\"\n",
    "    train_data = train_test_split(\n",
    "        X_numeric_values,\n",
    "        X_type_values,\n",
    "        y,\n",
    "        test_size=test_data_size,\n",
    "        random_state=666,  # Controls the shuffling applied to the data before applying the split\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        torch.from_numpy(arr) for arr in train_data\n",
    "    )  # Ja converto direto para Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2fc785",
   "metadata": {},
   "source": [
    "#### Divisão em Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7c577b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches_loader(\n",
    "    X_num_train: torch.Tensor, X_type_train: torch.Tensor, y_train: torch.Tensor, batch_size: int\n",
    ") -> DataLoader:\n",
    "    \"\"\"\"\"\"\n",
    "    train_dataset = TensorDataset(X_num_train, X_type_train, y_train)\n",
    "    return DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79078a6d",
   "metadata": {},
   "source": [
    "#### Obtendo dimeções em que vamos trabalhar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8ff5c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dimentions(\n",
    "    X_num_train: torch.Tensor, X_type_train: torch.Tensor\n",
    ") -> tuple[int, int]:\n",
    "    \"\"\"\"\"\"\n",
    "    single_w_size = X_num_train.shape[1]\n",
    "    w_type_size = X_type_train.shape[1]\n",
    "    return single_w_size, w_type_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be04dcff",
   "metadata": {},
   "source": [
    "#### Obetendo as informações necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebbb7225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_necessary_data(\n",
    "    df: pandas.DataFrame,\n",
    "    batch_size: int,\n",
    "    test_data_size: float,\n",
    ") -> tuple[int, int, DataLoader, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\"\"\"\n",
    "    X_numeric_values, X_type_values, y = filter_columns(df=df)\n",
    "\n",
    "    X_numeric_values = normalize_values(X_numeric_values=X_numeric_values)\n",
    "\n",
    "    X_num_train, X_num_test, X_type_train, X_type_test, y_train, y_test = (\n",
    "        get_train_test_sets(\n",
    "            X_numeric_values=X_numeric_values,\n",
    "            X_type_values=X_type_values,\n",
    "            y=y,\n",
    "            test_data_size=test_data_size,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    train_dataloader = get_batches_loader(\n",
    "        X_num_train=X_num_train,\n",
    "        X_type_train=X_type_train,\n",
    "        y_train=y_train,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    single_w_size, w_type_size = get_dimentions(\n",
    "        X_num_train=X_num_train, X_type_train=X_type_train\n",
    "    )\n",
    "\n",
    "    return single_w_size, w_type_size, train_dataloader, X_num_test, X_type_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348bf2ab",
   "metadata": {},
   "source": [
    "#### Função de treinamento\n",
    "\n",
    "Função de erro Mean Square Error\n",
    "\n",
    "$$\n",
    "MSE(\\vec{w}) = \\frac{1}{N}\\sum_{i=1}^{N}\\left(y^{i} - X_{j}^{i}\\ w^{j}\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b18f9d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.adam\n",
    "\n",
    "\n",
    "def train_in_epoch(\n",
    "    optimizer: torch.optim.SGD,\n",
    "    mse_loss: torch.nn.MSELoss,\n",
    "    w_tensor: torch.Tensor,\n",
    "    train_dataloader: DataLoader,\n",
    "    total_loss: float = 0.0,\n",
    ") -> float:\n",
    "    \"\"\"Treina o modelo dentro em\n",
    "    uma unica época\"\"\"\n",
    "    for x_num_batch, x_type_batch, y_batch in train_dataloader:\n",
    "        type_idx: torch.Tensor = x_type_batch.argmax(dim=1)  # Tensor (Batch Size,)\n",
    "\n",
    "        # Tensor (Weight Type Size, Single Weight Size) -> Tensor (Batch Size, Single Weigh Size)\n",
    "        w_sample = w_tensor[type_idx]\n",
    "\n",
    "        predict = (x_num_batch * w_sample).sum(dim=1, keepdim=True)  # Tensor (Batch Size, )\n",
    "        # predict = predict.unsqueeze(1)  # Tensor (Batch Size, 1)\n",
    "\n",
    "        loss: torch.Tensor = mse_loss(predict, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_([w_tensor], max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * x_num_batch.size(0)\n",
    "        \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    learning_rate: float | torch.Tensor,\n",
    "    n_epochs: int,\n",
    "    w_type_size: int,\n",
    "    single_w_size: int,\n",
    "    train_dataloader: DataLoader,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Treina o modelo dentro de varias épocas\"\"\"\n",
    "    # Cria Tensor de peso aleatorio\n",
    "    w_tensor = torch.randn(\n",
    "        w_type_size, single_w_size, requires_grad=True\n",
    "    )  # Tensor (2, 12), inclui o BIAS\n",
    "    # print(w_tensor.shape)\n",
    "\n",
    "    # Inicia optimizer e função de erro\n",
    "    optimizer = torch.optim.SGD([w_tensor], lr=learning_rate)\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "    # Roda por cada época\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = train_in_epoch(\n",
    "            optimizer=optimizer,\n",
    "            mse_loss=mse_loss,\n",
    "            w_tensor=w_tensor,\n",
    "            train_dataloader=train_dataloader,\n",
    "        )\n",
    "        train_mse = total_loss / len(train_dataloader.dataset)\n",
    "        if (epoch + 1) % 100 == 0: # Print every 10 epochs\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs:_}], Train Loss: {train_mse:.4f}')\n",
    "        \n",
    "    return w_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc4717b",
   "metadata": {},
   "source": [
    "#### Função de avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "026ee568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "def evaluate(\n",
    "    X_num_test: torch.Tensor,\n",
    "    X_type_test: torch.Tensor,\n",
    "    y_test: torch.Tensor,\n",
    "    w_tensor: torch.Tensor,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\"\"\"\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        type_idx = X_type_test.argmax(dim=1)  # Tensor(Test Size, 1)\n",
    "\n",
    "        w_sample = w_tensor[\n",
    "            type_idx\n",
    "        ]  # Tensor (Weight Type Size, Single Weight Size) -> Tensor (Test Size, Single Weight size)\n",
    "        \n",
    "        predict = (X_num_test * w_sample).sum(dim=1)\n",
    "        predict = predict.unsqueeze(1)\n",
    "        \n",
    "        loss: torch.Tensor = mse_loss(predict, y_test)\n",
    "        # Mean Square Error\n",
    "        mse = loss.item()\n",
    "        # Accuracy\n",
    "        accu = (predict.round() == y_test).float().mean().item()\n",
    "        # accu, _ = pearsonr(predict, y_test)\n",
    "        \n",
    "        return mse, accu\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c224cdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5613079071044922 0.510769248008728\n"
     ]
    }
   ],
   "source": [
    "single_w_size, w_type_size, train_dataloader, X_num_test, X_type_test, y_test = (\n",
    "    get_necessary_data(df=df, batch_size=64, test_data_size=0.2)\n",
    ")\n",
    "w_tensor = train_model(\n",
    "    learning_rate=1e-1,\n",
    "    n_epochs=10,\n",
    "    w_type_size=w_type_size,\n",
    "    single_w_size=single_w_size,\n",
    "    train_dataloader=train_dataloader,\n",
    ")\n",
    "\n",
    "mse, accu = evaluate(\n",
    "    X_num_test=X_num_test, X_type_test=X_type_test, y_test=y_test, w_tensor=w_tensor\n",
    ")\n",
    "\n",
    "print(mse, accu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
